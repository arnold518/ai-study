# Project Architecture - Korean-English NMT

**Status:** Minimal skeleton ready for Phase 1 implementation
**Current Phase:** 1.1 Complete (Data Download & Merging) âœ…

---

## Directory Structure

```
.
â”œâ”€â”€ scripts/              # Executable scripts
â”‚   â”œâ”€â”€ download_data.py      âœ… COMPLETE - Download datasets from HuggingFace
â”‚   â”œâ”€â”€ split_data.py         âœ… COMPLETE - Merge & clean datasets
â”‚   â”œâ”€â”€ train_tokenizer.py    ğŸ“ SKELETON - Train SentencePiece models
â”‚   â”œâ”€â”€ train.py              ğŸ“ SKELETON - Training pipeline
â”‚   â””â”€â”€ translate.py          ğŸ“ SKELETON - Inference pipeline
â”‚
â”œâ”€â”€ src/                  # Source code modules
â”‚   â”œâ”€â”€ data/             # Data processing
â”‚   â”‚   â”œâ”€â”€ tokenizer.py      ğŸ“ SKELETON - SentencePiece tokenizer wrapper
â”‚   â”‚   â””â”€â”€ dataset.py        ğŸ“ SKELETON - PyTorch Dataset for parallel corpus
â”‚   â”‚
â”‚   â”œâ”€â”€ models/           # Model architectures
â”‚   â”‚   â””â”€â”€ transformer/      ğŸ”§ EXISTING - Transformer components (need review)
â”‚   â”‚       â”œâ”€â”€ transformer.py
â”‚   â”‚       â”œâ”€â”€ attention.py
â”‚   â”‚       â”œâ”€â”€ encoder.py
â”‚   â”‚       â”œâ”€â”€ decoder.py
â”‚   â”‚       â”œâ”€â”€ feedforward.py
â”‚   â”‚       â”œâ”€â”€ positional_encoding.py
â”‚   â”‚       â””â”€â”€ embeddings.py
â”‚   â”‚
â”‚   â”œâ”€â”€ training/         # Training infrastructure
â”‚   â”‚   â”œâ”€â”€ trainer.py        ğŸ”§ EXISTING - Training loop (needs review)
â”‚   â”‚   â”œâ”€â”€ optimizer.py      ğŸ”§ EXISTING - Noam scheduler (needs review)
â”‚   â”‚   â””â”€â”€ losses.py         ğŸ”§ EXISTING - Label smoothing (needs review)
â”‚   â”‚
â”‚   â”œâ”€â”€ inference/        # Decoding strategies
â”‚   â”‚   â”œâ”€â”€ beam_search.py    ğŸ”§ EXISTING (needs review)
â”‚   â”‚   â”œâ”€â”€ greedy_search.py  ğŸ”§ EXISTING (needs review)
â”‚   â”‚   â””â”€â”€ translator.py     ğŸ”§ EXISTING (needs review)
â”‚   â”‚
â”‚   â””â”€â”€ utils/            # Helper functions
â”‚       â”œâ”€â”€ masking.py        ğŸ”§ EXISTING (needs review)
â”‚       â”œâ”€â”€ metrics.py        ğŸ”§ EXISTING (needs review)
â”‚       â””â”€â”€ checkpointing.py  ğŸ”§ EXISTING (needs review)
â”‚
â”œâ”€â”€ config/               # Configuration files
â”‚   â”œâ”€â”€ base_config.py        âœ… Shared settings
â”‚   â””â”€â”€ transformer_config.py âœ… Transformer hyperparameters
â”‚
â”œâ”€â”€ data/                 # Data storage
â”‚   â”œâ”€â”€ raw/              # Downloaded datasets (by source)
â”‚   â”‚   â”œâ”€â”€ moo/          # train/validation/test.{ko,en}
â”‚   â”‚   â”œâ”€â”€ tatoeba/      # validation/test.{ko,en}
â”‚   â”‚   â””â”€â”€ aihub/        # train.{ko,en}
â”‚   â”‚
â”‚   â”œâ”€â”€ processed/        # Unified, cleaned datasets âœ…
â”‚   â”‚   â”œâ”€â”€ train.{ko,en}         âœ… 897k pairs
â”‚   â”‚   â”œâ”€â”€ validation.{ko,en}    âœ… 1.9k pairs
â”‚   â”‚   â”œâ”€â”€ test.{ko,en}          âœ… 4k pairs
â”‚   â”‚   â””â”€â”€ statistics.json       âœ…
â”‚   â”‚
â”‚   â””â”€â”€ vocab/            # Tokenizer models (to be created)
â”‚       â”œâ”€â”€ ko_spm.model      ğŸ“ TODO
â”‚       â”œâ”€â”€ ko_spm.vocab      ğŸ“ TODO
â”‚       â”œâ”€â”€ en_spm.model      ğŸ“ TODO
â”‚       â””â”€â”€ en_spm.vocab      ğŸ“ TODO
â”‚
â”œâ”€â”€ checkpoints/          # Saved models
â”œâ”€â”€ logs/                 # Training logs
â””â”€â”€ outputs/              # Generated translations

Legend:
  âœ… COMPLETE   - Fully implemented and tested
  ğŸ“ SKELETON   - Minimal structure with TODOs
  ğŸ”§ EXISTING   - Previously created, needs review/testing
```

---

## Implementation Pipeline

### âœ… Phase 1.1: Data Acquisition (COMPLETE)

**Purpose:** Download and merge multiple datasets into unified splits

**Scripts:**
- `scripts/download_data.py` - Downloads Moo, Tatoeba, AIHub datasets
- `scripts/split_data.py` - Merges datasets, applies filtering, creates unified splits

**Usage:**
```bash
# Download datasets
/home/arnold/venv/bin/python scripts/download_data.py all

# Merge and clean
/home/arnold/venv/bin/python scripts/split_data.py
```

**Output:** `data/processed/train.{ko,en}`, `validation.{ko,en}`, `test.{ko,en}`

---

### ğŸ“ Phase 1.2: Tokenization (NEXT)

**Purpose:** Train subword tokenizers for Korean and English

**Key Module:** `src/data/tokenizer.py`
- Class: `SentencePieceTokenizer`
- Methods: `tokenize()`, `detokenize()`, `encode_ids()`, `decode_ids()`

**Script:** `scripts/train_tokenizer.py`

**Implementation Steps:**
1. Train SentencePiece model on `data/processed/train.ko` â†’ `data/vocab/ko_spm.model`
2. Train SentencePiece model on `data/processed/train.en` â†’ `data/vocab/en_spm.model`
3. Test tokenization on sample sentences

**Key Decisions:**
- Vocab size: 16,000 (configurable)
- Model type: Unigram (SentencePiece default)
- Character coverage: 0.9995 (for Korean)
- Special tokens: `<pad>=0, <unk>=1, <s>=2, </s>=3`

---

### ğŸ“ Phase 1.3: Dataset Implementation

**Purpose:** Create PyTorch Dataset for loading and batching

**Key Module:** `src/data/dataset.py`
- Class: `TranslationDataset`
- Function: `collate_fn()` for padding

**Implementation Steps:**
1. Load text files in `__init__`
2. Tokenize on-the-fly in `__getitem__` (or pre-tokenize)
3. Add BOS/EOS tokens
4. Implement `collate_fn` for padding
5. Test with DataLoader

**Data Flow:**
```
Text file â†’ Load â†’ Tokenize â†’ Add BOS/EOS â†’ Tensor â†’ Batch â†’ Pad â†’ Model
```

---

### ğŸ”§ Phase 2: Model & Training (REVIEW NEEDED)

**Purpose:** Implement Transformer architecture and training loop

**Key Modules:**
- `src/models/transformer/` - Model architecture
- `src/training/trainer.py` - Training loop
- `src/training/optimizer.py` - Noam learning rate scheduler
- `src/training/losses.py` - Label smoothing loss

**Status:** Components exist from ROADMAP template, need review and testing

**Implementation Steps:**
1. Review existing Transformer implementation
2. Test model forward pass with dummy data
3. Review training loop and optimizer
4. Start training on small subset
5. Scale to full dataset

---

### ğŸ”§ Phase 3: Inference (LATER)

**Purpose:** Generate translations from trained model

**Key Modules:**
- `src/inference/greedy_search.py` - Fast, simple decoding
- `src/inference/beam_search.py` - Better quality decoding
- `scripts/translate.py` - User interface

**Status:** Components exist, need implementation

---

## Data Flow Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Raw Datasets    â”‚ (download_data.py)
â”‚ - moo           â”‚
â”‚ - tatoeba       â”‚
â”‚ - aihub         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Merged Dataset  â”‚ (split_data.py)
â”‚ - train.ko/en   â”‚
â”‚ - val.ko/en     â”‚
â”‚ - test.ko/en    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Tokenizers      â”‚ (train_tokenizer.py) ğŸ“ NEXT
â”‚ - ko_spm.model  â”‚
â”‚ - en_spm.model  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PyTorch Dataset â”‚ (dataset.py) ğŸ“ NEXT
â”‚ TranslationData â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ DataLoader      â”‚ (train.py)
â”‚ Batching+Paddingâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Transformer     â”‚ (model/)
â”‚ Training        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Trained Model   â”‚
â”‚ Checkpoints     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Translation     â”‚ (translate.py)
â”‚ Inference       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Key Design Decisions

### 1. **Tokenization: SentencePiece (Unigram)**
**Why?**
- Language-agnostic (same approach for Korean & English)
- Handles OOV through subword decomposition
- Industry standard (used in production NMT)
- No preprocessing needed (handles raw text)

**Alternative considered:** Mecab + BPE (more complex, language-specific)

---

### 2. **Unified Dataset Structure**
**Why?**
- Combine multiple data sources for larger training set
- Single vocabulary and processing pipeline
- Easier to manage than per-source datasets

**Output:**
- `data/processed/` contains merged, cleaned splits
- Statistics track contribution from each source

---

### 3. **Vocabulary Handled by SentencePiece**
**Why?**
- SentencePiece has built-in vocabulary management
- `encode_ids()` returns token IDs directly
- Simpler than maintaining separate Vocabulary class

**Removed:** `src/data/vocabulary.py` (redundant)

---

### 4. **Configuration Hierarchy**
**Structure:**
- `config/base_config.py` - Shared settings (batch size, device, etc.)
- `config/transformer_config.py` - Model hyperparameters

**Benefits:** Easy to experiment with different configurations

---

## Implementation Priority

### Immediate (Phase 1.2-1.3):
1. âœ… `scripts/train_tokenizer.py` - Train SentencePiece
2. âœ… `src/data/tokenizer.py` - Implement wrapper
3. âœ… `src/data/dataset.py` - Implement Dataset
4. âœ… Test data pipeline end-to-end

### Soon (Phase 2):
5. Review `src/models/transformer/` components
6. Review `src/training/` components
7. Implement `scripts/train.py`
8. Start training

### Later (Phase 3):
9. Implement inference (`scripts/translate.py`)
10. Implement evaluation (BLEU scores)
11. Hyperparameter tuning

---

## Module Dependencies

```
scripts/train.py
    â”œâ”€â”€ config/transformer_config.py
    â”œâ”€â”€ src/data/tokenizer.py
    â”‚   â””â”€â”€ sentencepiece (external)
    â”œâ”€â”€ src/data/dataset.py
    â”‚   â””â”€â”€ src/data/tokenizer.py
    â”œâ”€â”€ src/models/transformer/transformer.py
    â”‚   â”œâ”€â”€ encoder.py â†’ attention.py, feedforward.py
    â”‚   â””â”€â”€ decoder.py â†’ attention.py, feedforward.py
    â””â”€â”€ src/training/trainer.py
        â”œâ”€â”€ optimizer.py
        â””â”€â”€ losses.py

scripts/translate.py
    â”œâ”€â”€ src/data/tokenizer.py
    â”œâ”€â”€ src/models/transformer/transformer.py
    â””â”€â”€ src/inference/beam_search.py (or greedy_search.py)
```

---

## Testing Strategy

### Unit Tests (per module):
- `src/data/tokenizer.py` â†’ Test encode/decode
- `src/data/dataset.py` â†’ Test loading and batching
- `src/models/transformer/` â†’ Test forward pass shapes

### Integration Tests:
- End-to-end data pipeline
- Training loop (single batch)
- Inference (dummy model)

### System Tests:
- Train on small dataset (1000 samples)
- Evaluate BLEU on test set
- Compare with baseline

---

## Next Steps

**Immediate TODO (Phase 1.2):**

1. **Implement `scripts/train_tokenizer.py`:**
   ```python
   import sentencepiece as spm

   spm.SentencePieceTrainer.train(
       input='data/processed/train.ko',
       model_prefix='data/vocab/ko_spm',
       vocab_size=16000,
       ...
   )
   ```

2. **Implement `src/data/tokenizer.py`:**
   ```python
   class SentencePieceTokenizer:
       def __init__(self, model_path):
           self.sp = spm.SentencePieceProcessor(model_file=model_path)

       def encode_ids(self, text):
           return self.sp.encode(text, out_type=int)
   ```

3. **Implement `src/data/dataset.py`:**
   ```python
   def __getitem__(self, idx):
       src_ids = self.src_tokenizer.encode_ids(self.src_lines[idx])
       src_ids = [BOS] + src_ids + [EOS]
       return torch.tensor(src_ids)
   ```

4. **Test pipeline:**
   ```bash
   /home/arnold/venv/bin/python scripts/train_tokenizer.py
   /home/arnold/venv/bin/python scripts/train.py  # Should load data
   ```

---

## Questions to Resolve

1. **Shared vs Separate Vocabularies?**
   - Current: Separate (ko_spm, en_spm)
   - Alternative: Shared vocabulary (single model)

2. **Pre-tokenize or On-the-fly?**
   - Current skeleton: On-the-fly in `__getitem__`
   - Alternative: Pre-tokenize and save token IDs

3. **Maximum sequence length?**
   - Current: 5000 in config (for positional encoding)
   - Training: Could use 150 (from split_data filter)

4. **Batch size?**
   - Need to determine based on GPU memory
   - Start with small (16-32) and increase

---

**Status:** Ready to implement Phase 1.2 (Tokenization)
