# Transformer NMT Improvement Recommendations

**Document Version**: 1.0
**Date**: 2025-12-12
**Status**: Korean-English Neural Machine Translation System

---

## Executive Summary

This document provides comprehensive recommendations to improve translation quality, focusing on:
1. **Repetition Problems**: Repeated word sequences in output
2. **Number Hallucinations**: Incorrect numbers in translations
3. **Overall Quality**: BLEU score and fluency improvements

**Current Issues Identified**:
- ‚úÖ Double dropout bug (FIXED)
- ‚ö†Ô∏è Shared embeddings for linguistically distant languages
- ‚ö†Ô∏è Repetition in generated sequences
- ‚ö†Ô∏è Number hallucinations and inaccuracies

**Expected Improvements**:
- 3-5 BLEU points from configuration optimizations
- 80% reduction in number errors with copy mechanism
- 60% reduction in repetitions with enhanced penalties
- 2-3x faster training with mixed precision

---

## ‚úÖ Implementation Status Checklist

**Last Updated**: 2025-12-12

### üü¢ Completed (Ready for Training)

#### Critical Bug Fixes
- ‚úÖ **Double Dropout Bug** - Fixed in feedforward.py (removed internal dropout)
- ‚úÖ **Cross-Attention Mask** - Fixed shape issues in inference files

#### Configuration Improvements (Phase 1)
- ‚úÖ **1.1** Separate Source-Target Embeddings (`share_src_tgt_embed = False`)
- ‚úÖ **1.2** Reduce Dropout to 0.1 (from 0.15)
- ‚úÖ **1.3** Fix Warmup Steps to 4000 (from 16000)
- ‚úÖ **1.4** Gradient Accumulation (effective batch size = 256)
- ‚úÖ **1.5** Increase Sequence Length to 150 (from 128)
- ‚úÖ **1.6** Increase Learning Rate Factor to 2.0 (bonus optimization)

#### Repetition Solutions (Phase 2)
- ‚úÖ **2.1** Increase Repetition Penalty (1.5 from 1.2, window 30 from 20)
- ‚úÖ **2.4** Diverse Beam Search (8 beams, 4 groups, diversity penalty 0.5)
- ‚è≠Ô∏è **2.2** N-gram Blocking (SKIPPED - outdated approach, using 2.1 + 2.4 instead)
- üî≤ **2.3** Coverage Penalty (optional, not implemented)
- üî≤ **2.5** Temperature Sampling (optional, not implemented)

#### Training Optimizations (Phase 5)
- ‚úÖ **5.1** Mixed Precision Training (AMP enabled, 2-3x speedup expected)
- ‚úÖ **5.2** Gradient Accumulation (already implemented with 1.4)
- üî≤ **5.3** Learning Rate Schedules (optional alternatives)
- üî≤ **5.4** Back-Translation (data augmentation, not implemented)
- üî≤ **5.5** Curriculum Learning (optional, not implemented)

### üü° Partially Implemented / In Progress

#### Number Hallucination Solutions (Phase 3)
- üî≤ **3.1** Copy Mechanism (HIGH PRIORITY - not implemented yet)
- üî≤ **3.2** Number Placeholder System (alternative to 3.1)
- üî≤ **3.3** Number-Aware Loss (not implemented)
- üî≤ **3.4** Number Alignment Post-Processing (easy win, not implemented)
- üî≤ **3.5** NER Protection (advanced, not implemented)

#### Architecture Improvements (Phase 4)
- üî≤ **4.1** Pre-Layer Normalization (optional, not implemented)
- üî≤ **4.2** Relative Positional Encoding (advanced, not implemented)
- üî≤ **4.3** Learned Positional Encoding (alternative, not implemented)

#### Data Quality (Phase 7)
- üî≤ **7.1** Advanced Data Cleaning (recommended)
- üî≤ **7.2** Increase Vocabulary Size to 32k (not implemented)
- üî≤ **7.3** Try BPE Tokenization (experiment)
- üî≤ **7.4** Subword Regularization (advanced)

#### Inference Improvements (Phase 6)
- üî≤ **6.1** Ensemble Decoding (not implemented)
- üî≤ **6.2** Checkpoint Averaging (easy win, not implemented)
- üî≤ **6.3** Minimum Length Constraint (not implemented)
- üî≤ **6.4** Length Normalization Tuning (experiment)

#### Evaluation & Monitoring (Phase 8)
- ‚úÖ **8.1** Better Metrics (chrF++, COMET, BERTScore) - Implemented in metrics.py
- ‚úÖ **8.2** Error Analysis Tools - Implemented in error_analysis.py
- ‚úÖ **8.3** Attention Visualization - Implemented in visualization.py
- ‚úÖ **8.4** Gradient Monitoring - Integrated into trainer.py

### üìä Summary Statistics

**Total Recommendations**: 44
**Implemented**: 15 ‚úÖ (34%)
**In Progress**: 0 üü°
**Not Implemented**: 29 üî≤
**Skipped (Intentional)**: 1 ‚è≠Ô∏è

**Latest Update**: Added evaluation and monitoring tools (8.1-8.4) - 2025-12-12

**Expected BLEU Improvement from Completed**: +3-5 points
**Expected Speed Improvement**: 2-3x faster
**Expected Repetition Reduction**: 70-90%

### üéØ Recommended Next Steps

**Immediate (Do Now)**:
1. ‚úÖ **DONE** - All Phase 1 & 2 configurations implemented
2. ‚úÖ **DONE** - Mixed precision training enabled
3. üöÄ **NEXT** - Retrain model with new configurations
4. üìä **NEXT** - Evaluate BLEU scores and repetition rates

**Short-term (After Initial Training)**:
1. ‚úÖ **DONE** - Better Metrics (8.1-8.4) - Evaluation tools implemented
2. üî≤ Implement Checkpoint Averaging (6.2) - Easy 0.5 BLEU boost
3. üî≤ Implement Number Alignment Post-Processing (3.4) - Quick fix for numbers

**Medium-term (If Number Problems Persist)**:
1. üî≤ Implement Copy Mechanism (3.1) - Most effective for numbers
2. üî≤ Advanced Data Cleaning (7.1) - Improve data quality

**Long-term (For Production)**:
1. üî≤ Ensemble Decoding (6.1) - Best quality
2. üî≤ Increase Vocabulary (7.2) - Better coverage
3. üî≤ Pre-Layer Normalization (4.1) - Architecture improvement

---

## Table of Contents

1. [Critical Configuration Fixes](#1-critical-configuration-fixes)
2. [Repetition Solutions](#2-repetition-solutions)
3. [Number Hallucination Solutions](#3-number-hallucination-solutions)
4. [Architecture Improvements](#4-architecture-improvements)
5. [Training Optimizations](#5-training-optimizations)
6. [Inference Improvements](#6-inference-improvements)
7. [Data Quality](#7-data-quality)
8. [Evaluation & Monitoring](#8-evaluation--monitoring)
9. [Implementation Roadmap](#9-implementation-roadmap)

---

## 1. Critical Configuration Fixes

### 1.1 Separate Source-Target Embeddings ‚≠ê PRIORITY 1

**Current Setting**:
```python
# config/transformer_config.py
share_src_tgt_embed = True  # ‚ùå Wrong for Korean-English
```

**Recommended**:
```python
share_src_tgt_embed = False  # ‚úÖ Separate embeddings for distant languages
tie_embeddings = True         # ‚úÖ Keep this (ties target embedding with output)
```

**Rationale**:
- Korean and English are linguistically distant (different families)
- Shared embeddings force both into same space
- Separate embeddings allow language-specific features

**Expected Impact**: +0.5-1.0 BLEU, better handling of Korean-specific grammar

**Implementation**:
- File: `config/transformer_config.py:21`
- Change value, retrain model
- Note: Requires separate vocabularies (`use_shared_vocab = False` in base_config.py)

---

### 1.2 Reduce Dropout Rate ‚≠ê PRIORITY 1

**Current Setting**:
```python
# config/base_config.py
dropout = 0.15  # Too high after fixing double dropout bug
```

**Recommended**:
```python
dropout = 0.1  # Match original paper
```

**Rationale**:
- Paper uses 0.1 dropout
- We fixed double dropout bug, so 0.15 is now too aggressive
- May be causing underfitting

**Expected Impact**: +0.5-1.0 BLEU, faster convergence

**Implementation**:
- File: `config/base_config.py:34`
- Change value and retrain

---

### 1.3 Fix Warmup Steps ‚≠ê PRIORITY 1

**Current Setting**:
```python
# config/transformer_config.py
warmup_steps = 16000  # Too many for dataset size
```

**Recommended**:
```python
warmup_steps = 4000  # Paper default, good for 897k dataset
```

**Rationale**:
- Paper uses 4000 for 4.5M sentences
- Your dataset: 897k sentences
- 16000 steps = learning rate stays low too long
- Formula: ~0.5-1.0 epochs worth of steps

**Expected Impact**: Better early training, +0.3-0.5 BLEU

**Implementation**:
- File: `config/transformer_config.py:25`
- Change value and retrain

---

### 1.4 Increase Batch Size ‚≠ê PRIORITY 2

**Current Setting**:
```python
# config/base_config.py
batch_size = 128
```

**Recommended**:
```python
batch_size = 256  # If GPU memory allows
# OR use gradient accumulation:
gradient_accumulation_steps = 2  # Effective batch = 256
gradient_accumulation_steps = 4  # Effective batch = 512
```

**Rationale**:
- Larger batches ‚Üí more stable training
- Paper uses batch size up to 4096 tokens per batch
- Can simulate with gradient accumulation

**Expected Impact**: +0.5-1.0 BLEU, faster training

**Implementation**:
- File: `config/base_config.py:28`
- Or add gradient accumulation to `training/trainer.py`

---

### 1.5 Increase Sequence Length ‚≠ê PRIORITY 2

**Current Setting**:
```python
# config/base_config.py
max_seq_length = 128
```

**Recommended**:
```python
max_seq_length = 150  # Allow longer sentences
```

**Rationale**:
- Korean sentences can be longer
- 128 tokens may truncate important information
- Better to be conservative

**Expected Impact**: Better handling of long sentences, +0.2-0.3 BLEU

---

### 1.6 Increase Learning Rate Factor (Optional)

**Current Setting**:
```python
# config/transformer_config.py
learning_rate = 1.0  # Factor in Noam schedule
```

**Recommended**:
```python
learning_rate = 2.0  # Try higher factor for smaller dataset
```

**Rationale**:
- Smaller datasets often benefit from higher learning rates
- Experiment: try 1.0, 1.5, 2.0

**Expected Impact**: Potentially faster convergence

---

## 2. Repetition Solutions

### 2.1 Increase Repetition Penalty ‚≠ê PRIORITY 1

**Current Implementation**:
```python
# src/inference/greedy_search.py:91
next_token_logits = apply_repetition_penalty(
    next_token_logits, tgt, penalty=1.2, window_size=20
)
```

**Recommended**:
```python
penalty = 1.5  # Increase from 1.2 (stronger penalty)
window_size = 30  # Increase from 20 (longer memory)
```

**Implementation**:
- Files: `src/inference/greedy_search.py:91`, `beam_search.py:128`
- Add as config parameters:
  ```python
  # config/transformer_config.py
  repetition_penalty = 1.5
  repetition_window = 30
  ```

**Expected Impact**: 40-60% reduction in repetitions

---

### 2.2 N-gram Blocking üÜï PRIORITY 2

**New Feature**: Block tokens that would create repeated n-grams

**Implementation**:
```python
# src/inference/ngram_blocking.py (NEW FILE)
def apply_ngram_blocking(logits, generated_ids, ngram_size=3):
    """
    Block tokens that would create repeated n-grams.

    Args:
        logits: [vocab_size] token logits
        generated_ids: List of previously generated token IDs
        ngram_size: Size of n-grams to check (default: 3 for trigrams)

    Returns:
        logits: Modified logits with blocked tokens set to -inf
    """
    if len(generated_ids) < ngram_size:
        return logits

    # Get last ngram_size-1 tokens
    context = tuple(generated_ids[-(ngram_size-1):])

    # Find all n-grams in history
    seen_ngrams = set()
    for i in range(len(generated_ids) - ngram_size + 1):
        ngram = tuple(generated_ids[i:i+ngram_size])
        seen_ngrams.add(ngram)

    # Block tokens that would complete a repeated n-gram
    for token_id in range(len(logits)):
        candidate_ngram = context + (token_id,)
        if candidate_ngram in seen_ngrams:
            logits[token_id] = float('-inf')

    return logits
```

**Integration**:
```python
# In greedy_search.py and beam_search.py
from .ngram_blocking import apply_ngram_blocking

# After repetition penalty:
next_token_logits = apply_ngram_blocking(
    next_token_logits, tgt.tolist()[0], ngram_size=3
)
```

**Config**:
```python
# config/transformer_config.py
use_ngram_blocking = True
ngram_block_size = 3  # Trigram blocking
```

**Expected Impact**: 70-90% reduction in phrase repetitions

---

### 2.3 Coverage Penalty üÜï PRIORITY 3

**New Feature**: Track attention coverage to prevent repetition

**Implementation**:
```python
# src/inference/coverage.py (NEW FILE)
import torch

class CoverageMechanism:
    """Track attention coverage during decoding."""

    def __init__(self, src_len):
        self.coverage = torch.zeros(src_len)
        self.beta = 0.5  # Coverage penalty weight

    def update(self, attention_weights):
        """
        Update coverage vector.

        Args:
            attention_weights: [src_len] attention distribution
        """
        penalty = torch.sum(torch.min(attention_weights, self.coverage))
        self.coverage = self.coverage + attention_weights
        return penalty

    def get_penalty(self):
        """Get coverage penalty to subtract from score."""
        return self.beta * self.coverage.sum()
```

**Integration**: Requires modification to beam search to track attention

**Expected Impact**: Reduces attention repetition, +0.2-0.3 BLEU

---

### 2.4 Diverse Beam Search üÜï PRIORITY 3

**New Feature**: Use diverse beam groups

**Implementation**:
```python
# config/transformer_config.py
beam_size = 8  # Increase from 4
num_beam_groups = 4  # Divide into groups
diversity_penalty = 0.5  # Penalize similar hypotheses within timestep
```

**Algorithm**:
```python
# In beam_search.py
# Divide beams into groups
# Add diversity penalty to beam scores in same timestep
# Formula: score - diversity_penalty * (number of same token in group)
```

**Expected Impact**: More diverse outputs, fewer repetitions

---

### 2.5 Temperature Sampling üÜï PRIORITY 4

**New Feature**: Alternative to greedy decoding

**Implementation**:
```python
# src/inference/temperature_sampling.py (NEW FILE)
def sample_with_temperature(logits, temperature=0.7):
    """
    Sample from distribution with temperature.

    Args:
        logits: [vocab_size] token logits
        temperature: Lower = more conservative, higher = more random
            0.7 = good default for translation
            1.0 = standard sampling
            0.1 = almost greedy

    Returns:
        token_id: Sampled token
    """
    logits = logits / temperature
    probs = torch.softmax(logits, dim=-1)
    token_id = torch.multinomial(probs, 1)
    return token_id.item()
```

**Expected Impact**: More natural, less repetitive outputs

---

## 3. Number Hallucination Solutions

### 3.1 Copy Mechanism ‚≠ê‚≠ê‚≠ê HIGHEST PRIORITY

**New Feature**: Pointer-generator network for copying numbers

**Why This Is Critical**:
- 80% of number errors can be fixed by copying
- Most effective solution for factual accuracy
- Industry standard for NMT systems

**Architecture**:
```python
# src/models/transformer/copy_mechanism.py (NEW FILE)
import torch
import torch.nn as nn

class CopyMechanism(nn.Module):
    """
    Pointer-generator network for copying tokens from source.
    See: "Get To The Point" (See et al., 2017)
    """

    def __init__(self, d_model):
        super().__init__()
        self.p_gen_linear = nn.Linear(d_model * 3, 1)  # context + decoder_state + decoder_input

    def forward(self, decoder_output, encoder_output, attention_weights,
                decoder_input_emb, src_tokens, vocab_size):
        """
        Compute final distribution mixing generation and copying.

        Args:
            decoder_output: [batch, tgt_len, d_model] decoder hidden states
            encoder_output: [batch, src_len, d_model] encoder hidden states
            attention_weights: [batch, tgt_len, src_len] attention distributions
            decoder_input_emb: [batch, tgt_len, d_model] decoder input embeddings
            src_tokens: [batch, src_len] source token IDs
            vocab_size: Size of vocabulary

        Returns:
            final_dist: [batch, tgt_len, extended_vocab] final probability distribution
        """
        batch_size, tgt_len, _ = decoder_output.size()
        src_len = encoder_output.size(1)

        # Compute context vector from attention
        context = torch.bmm(attention_weights, encoder_output)  # [batch, tgt_len, d_model]

        # Compute generation probability p_gen
        p_gen_input = torch.cat([context, decoder_output, decoder_input_emb], dim=-1)
        p_gen = torch.sigmoid(self.p_gen_linear(p_gen_input))  # [batch, tgt_len, 1]

        # Generation distribution (from decoder)
        gen_dist = torch.softmax(decoder_output @ self.vocab_projection.weight.T, dim=-1)
        gen_dist = p_gen * gen_dist  # [batch, tgt_len, vocab_size]

        # Copy distribution (from attention)
        copy_dist = (1 - p_gen) * attention_weights  # [batch, tgt_len, src_len]

        # Create extended vocabulary distribution
        # Extended vocab = original vocab + OOV words from source
        extended_vocab_size = vocab_size + src_len
        final_dist = torch.zeros(batch_size, tgt_len, extended_vocab_size,
                                device=decoder_output.device)

        # Add generation probabilities
        final_dist[:, :, :vocab_size] = gen_dist

        # Add copy probabilities (scatter to corresponding token positions)
        final_dist.scatter_add_(2, src_tokens.unsqueeze(1).expand(-1, tgt_len, -1), copy_dist)

        return final_dist
```

**Integration into Transformer**:
```python
# src/models/transformer/transformer.py
class Transformer(nn.Module):
    def __init__(self, config, src_vocab_size, tgt_vocab_size, pad_idx=0):
        # ... existing code ...

        # Add copy mechanism
        if config.use_copy_mechanism:
            self.copy_mechanism = CopyMechanism(config.d_model)

    def forward(self, src, tgt, src_mask=None, tgt_mask=None, cross_mask=None):
        # ... existing encoder/decoder code ...

        # Get attention weights from last decoder layer
        # (Need to modify decoder to return attention weights)

        if self.config.use_copy_mechanism:
            logits = self.copy_mechanism(
                decoder_output, encoder_output, attention_weights,
                tgt_embedded, src, self.tgt_vocab_size
            )
        else:
            logits = self.output_projection(decoder_output)

        return logits
```

**Config**:
```python
# config/transformer_config.py
use_copy_mechanism = True  # Enable copy mechanism
copy_mechanism_weight = 1.0  # Weight for copy vs generate
```

**Expected Impact**:
- 70-80% reduction in number hallucinations
- Better handling of named entities
- +1.0-2.0 BLEU improvement

**Implementation Complexity**: HIGH (requires significant modifications)

---

### 3.2 Number Placeholder System üÜï PRIORITY 2

**New Feature**: Replace numbers with placeholders during training

**Preprocessing**:
```python
# src/data/number_replacement.py (NEW FILE)
import re

class NumberPlaceholder:
    """Replace numbers with placeholders for training."""

    def __init__(self):
        self.number_pattern = re.compile(r'\d+(?:\.\d+)?')
        self.placeholder_prefix = "<NUM"

    def replace_numbers(self, text):
        """
        Replace numbers with placeholders.

        Args:
            text: Input text

        Returns:
            processed_text: Text with placeholders
            number_map: Dict mapping placeholder to original number
        """
        number_map = {}
        counter = 0

        def replace_func(match):
            nonlocal counter
            number = match.group(0)
            placeholder = f"{self.placeholder_prefix}{counter}>"
            number_map[placeholder] = number
            counter += 1
            return placeholder

        processed_text = self.number_pattern.sub(replace_func, text)
        return processed_text, number_map

    def restore_numbers(self, text, number_map):
        """Restore original numbers from placeholders."""
        for placeholder, number in number_map.items():
            text = text.replace(placeholder, number)
        return text

# Example usage:
# Source: "2023ÎÖÑ 12ÏõîÏóê 100Îã¨Îü¨" ‚Üí "<NUM0>ÎÖÑ <NUM1>ÏõîÏóê <NUM2>Îã¨Îü¨"
# Target: "100 dollars in December 2023" ‚Üí "<NUM2> dollars in <NUM1> <NUM0>"
# Model learns structural mapping, not specific numbers
```

**Integration**:
```python
# src/data/dataset.py
class TranslationDataset(Dataset):
    def __init__(self, ..., use_number_placeholders=False):
        self.use_number_placeholders = use_number_placeholders
        if use_number_placeholders:
            self.number_replacer = NumberPlaceholder()

    def __getitem__(self, idx):
        src_text, tgt_text = self.load_pair(idx)

        if self.use_number_placeholders:
            src_text, src_map = self.number_replacer.replace_numbers(src_text)
            tgt_text, tgt_map = self.number_replacer.replace_numbers(tgt_text)
            # Store maps for later restoration

        # ... tokenization ...
```

**Expected Impact**: 50-60% reduction in number errors, simpler than copy mechanism

---

### 3.3 Number-Aware Loss üÜï PRIORITY 3

**New Feature**: Higher loss weight for number tokens

**Implementation**:
```python
# src/training/losses.py
class NumberAwareLoss(nn.Module):
    """Apply higher loss weight to number tokens."""

    def __init__(self, vocab_size, pad_idx, smoothing=0.1, number_weight=2.0):
        super().__init__()
        self.base_loss = LabelSmoothingLoss(vocab_size, pad_idx, smoothing)
        self.number_weight = number_weight
        self.number_pattern = re.compile(r'\d')

    def is_number_token(self, token_id, tokenizer):
        """Check if token contains digits."""
        token = tokenizer.decode_ids([token_id])
        return bool(self.number_pattern.search(token))

    def forward(self, logits, targets, tokenizer):
        """
        Compute weighted loss.

        Args:
            logits: [batch*seq_len, vocab_size]
            targets: [batch*seq_len]
            tokenizer: To decode token IDs
        """
        # Compute base loss per token
        base_loss = self.base_loss(logits, targets)

        # Create weight mask (higher weight for numbers)
        weights = torch.ones_like(targets, dtype=torch.float)
        for i, token_id in enumerate(targets):
            if self.is_number_token(token_id, tokenizer):
                weights[i] = self.number_weight

        # Apply weights
        weighted_loss = base_loss * weights
        return weighted_loss.mean()
```

**Expected Impact**: Model pays more attention to numbers during training

---

### 3.4 Number Alignment Post-Processing üÜï PRIORITY 2

**New Feature**: Post-process translations to fix number mismatches

**Implementation**:
```python
# src/inference/number_alignment.py (NEW FILE)
import re
from difflib import SequenceMatcher

class NumberAligner:
    """Align numbers between source and target."""

    def __init__(self):
        self.number_pattern = re.compile(r'\d+(?:\.\d+)?')

    def extract_numbers(self, text):
        """Extract all numbers from text."""
        return self.number_pattern.findall(text)

    def align_numbers(self, source_text, target_text):
        """
        Fix number mismatches in target based on source.

        Args:
            source_text: Source sentence
            target_text: Generated translation

        Returns:
            corrected_text: Target with corrected numbers
        """
        src_numbers = self.extract_numbers(source_text)
        tgt_numbers = self.extract_numbers(target_text)

        if not src_numbers:
            return target_text  # No numbers in source

        if len(tgt_numbers) == 0:
            return target_text  # No numbers generated

        # If counts match and numbers are similar, assume order is correct
        if len(src_numbers) == len(tgt_numbers):
            corrected = target_text
            for src_num, tgt_num in zip(src_numbers, tgt_numbers):
                # Replace if difference is large (likely hallucination)
                if abs(float(src_num) - float(tgt_num)) > 0.1 * float(src_num):
                    corrected = corrected.replace(tgt_num, src_num, 1)
            return corrected

        # If counts don't match, use heuristics or skip
        return target_text

    def strict_align(self, source_text, target_text):
        """Replace ALL numbers in target with source numbers (strict)."""
        src_numbers = self.extract_numbers(source_text)
        tgt_numbers = self.extract_numbers(target_text)

        if not src_numbers:
            return target_text

        # Replace in order
        corrected = target_text
        for i, tgt_num in enumerate(tgt_numbers):
            if i < len(src_numbers):
                corrected = corrected.replace(tgt_num, src_numbers[i], 1)

        return corrected
```

**Integration**:
```python
# src/inference/translator.py
class Translator:
    def translate(self, source_text, align_numbers=True):
        # ... existing translation code ...

        if align_numbers:
            aligner = NumberAligner()
            translation = aligner.align_numbers(source_text, translation)

        return translation
```

**Expected Impact**: 30-40% additional reduction in number errors (as post-processing)

---

### 3.5 Named Entity Recognition (NER) üÜï PRIORITY 4

**New Feature**: Protect numbers and named entities

**Approach**:
1. Use NER tool to identify numbers, dates, names
2. Treat as special tokens during training
3. Optionally copy directly during inference

**Expected Impact**: Better handling of factual information

---

## 4. Architecture Improvements

### 4.1 Pre-Layer Normalization (Pre-LN) üîß OPTIONAL

**Current**: Post-LN (x = LayerNorm(x + Sublayer(x)))

**Proposed**: Pre-LN (x = x + Sublayer(LayerNorm(x)))

**Implementation**:
```python
# src/models/transformer/encoder.py
class EncoderLayer(nn.Module):
    def forward(self, x, mask=None):
        # Pre-LN variant
        # Self-attention with pre-normalization
        normed = self.norm1(x)
        attn_output, _, _ = self.self_attn(normed, normed, normed, mask)
        x = x + self.dropout1(attn_output)

        # FFN with pre-normalization
        normed = self.norm2(x)
        ffn_output = self.ffn(normed)
        x = x + self.dropout2(ffn_output)

        return x
```

**Benefits**:
- More stable training (less gradient issues)
- Better for deeper models (>12 layers)
- Used in GPT-2, GPT-3, modern transformers

**Tradeoff**: Slight difference from original paper

**Expected Impact**: +0.3-0.5 BLEU for larger models

---

### 4.2 Relative Positional Encoding üÜï ADVANCED

**Proposed**: T5-style relative position bias

**Benefits**: Better generalization to longer sequences

**Complexity**: HIGH, requires significant changes

---

### 4.3 Learned Positional Encoding üîß ALTERNATIVE

**Current**: Sinusoidal (fixed)

**Proposed**: Learned embeddings

```python
# src/models/transformer/positional_encoding.py
class LearnedPositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=5000):
        super().__init__()
        self.pe = nn.Embedding(max_len, d_model)

    def forward(self, x, position_offset=0):
        seq_len = x.size(1)
        positions = torch.arange(position_offset, position_offset + seq_len,
                                device=x.device)
        return x + self.pe(positions).unsqueeze(0)
```

**Expected Impact**: Sometimes better than sinusoidal, worth experimenting

---

## 5. Training Optimizations

### 5.1 Mixed Precision Training ‚≠ê‚≠ê‚≠ê CRITICAL

**Why**: 2-3x faster training, 40% less GPU memory

**Implementation**:
```python
# src/training/trainer.py
from torch.cuda.amp import autocast, GradScaler

class Trainer:
    def __init__(self, ...):
        self.scaler = GradScaler()
        self.use_amp = True  # Automatic Mixed Precision

    def train_step(self, batch):
        self.optimizer.zero_grad()

        # Forward pass in mixed precision
        with autocast(enabled=self.use_amp):
            logits = self.model(batch['src'], batch['tgt'], ...)
            loss = self.criterion(logits, batch['tgt_y'])

        # Backward pass with gradient scaling
        self.scaler.scale(loss).backward()

        # Gradient clipping
        self.scaler.unscale_(self.optimizer)
        torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.grad_clip)

        # Optimizer step
        self.scaler.step(self.optimizer)
        self.scaler.update()

        return loss.item()
```

**Expected Impact**:
- 2-3x training speedup
- No quality loss
- Must have for production

---

### 5.2 Gradient Accumulation ‚≠ê PRIORITY 1

**Why**: Simulate larger batch sizes

**Implementation**:
```python
# src/training/trainer.py
class Trainer:
    def __init__(self, config):
        self.accumulation_steps = config.gradient_accumulation_steps  # e.g., 4

    def train_epoch(self, train_loader):
        self.optimizer.zero_grad()

        for step, batch in enumerate(train_loader):
            loss = self.train_step(batch)
            loss = loss / self.accumulation_steps  # Scale loss
            loss.backward()

            if (step + 1) % self.accumulation_steps == 0:
                # Gradient clipping
                torch.nn.utils.clip_grad_norm_(self.model.parameters(),
                                              self.config.grad_clip)
                # Update weights
                self.optimizer.step()
                self.optimizer.zero_grad()
```

**Config**:
```python
# config/base_config.py
gradient_accumulation_steps = 4  # Effective batch = 128 * 4 = 512
```

**Expected Impact**: Better training stability, +0.5-1.0 BLEU

---

### 5.3 Learning Rate Schedules üîß OPTIONAL

**Current**: Noam scheduler only

**Additional Options**:

```python
# src/training/optimizer.py
class WarmupThenCosine:
    """Warmup + Cosine annealing (popular in modern models)."""

    def __init__(self, optimizer, warmup_steps, total_steps):
        self.optimizer = optimizer
        self.warmup_steps = warmup_steps
        self.total_steps = total_steps
        self.step_num = 0

    def step(self):
        self.step_num += 1

        if self.step_num < self.warmup_steps:
            # Linear warmup
            lr = self.step_num / self.warmup_steps
        else:
            # Cosine decay
            progress = (self.step_num - self.warmup_steps) / (self.total_steps - self.warmup_steps)
            lr = 0.5 * (1 + math.cos(math.pi * progress))

        for param_group in self.optimizer.param_groups:
            param_group['lr'] = lr

        self.optimizer.step()
```

---

### 5.4 Back-Translation üÜï DATA AUGMENTATION

**Approach**:
1. Train reverse model (En‚ÜíKo)
2. Translate English monolingual data to Korean
3. Use synthetic Ko-En pairs as additional training data

**Expected Impact**: +1.0-2.0 BLEU with large monolingual data

---

### 5.5 Curriculum Learning üÜï OPTIONAL

**Approach**: Start with easy (short) sentences, gradually increase difficulty

```python
# src/data/curriculum.py
class CurriculumDataset:
    def __init__(self, dataset, epoch):
        self.dataset = dataset
        self.max_length = self.get_max_length_for_epoch(epoch)

    def get_max_length_for_epoch(self, epoch):
        if epoch < 5:
            return 50  # Short sentences first
        elif epoch < 10:
            return 100
        else:
            return 150  # Full length
```

**Expected Impact**: Faster initial learning

---

## 6. Inference Improvements

### 6.1 Ensemble Decoding ‚≠ê PRIORITY 2

**Approach**: Average predictions from multiple checkpoints

**Implementation**:
```python
# src/inference/ensemble.py (NEW FILE)
class EnsembleTranslator:
    """Ensemble multiple models for better predictions."""

    def __init__(self, model_paths, config, device):
        """
        Load multiple checkpoints.

        Args:
            model_paths: List of checkpoint paths
            config: Model config
            device: Device to run on
        """
        self.models = []
        for path in model_paths:
            model = load_model(path, config, device)
            model.eval()
            self.models.append(model)

    def translate(self, src, src_mask, max_length, bos_idx, eos_idx):
        """
        Ensemble decoding.

        Strategy: Average logits from all models at each step.
        """
        # Encode source with all models
        encoder_outputs = [model.encode(src, src_mask) for model in self.models]

        # Initialize decoding
        batch_size = src.size(0)
        tgt = torch.full((batch_size, 1), bos_idx, device=src.device)

        for step in range(max_length):
            # Get logits from all models
            all_logits = []
            for i, model in enumerate(self.models):
                tgt_mask = create_target_mask(tgt, pad_idx=0)
                cross_mask = create_cross_attention_mask(src, tgt, pad_idx=0)
                logits = model.decode(tgt, encoder_outputs[i], cross_mask, tgt_mask)
                all_logits.append(logits[:, -1, :])  # Last position

            # Average logits (or average probabilities)
            avg_logits = torch.stack(all_logits).mean(dim=0)
            next_token = avg_logits.argmax(dim=-1)

            # Append
            tgt = torch.cat([tgt, next_token.unsqueeze(1)], dim=1)

            # Check EOS
            if (next_token == eos_idx).all():
                break

        return tgt
```

**Usage**:
```python
# Use last 3-5 checkpoints
checkpoints = [
    'checkpoints/checkpoint_epoch25.pt',
    'checkpoints/checkpoint_epoch26.pt',
    'checkpoints/checkpoint_epoch27.pt',
]
ensemble = EnsembleTranslator(checkpoints, config, device)
translation = ensemble.translate(src, ...)
```

**Expected Impact**: +1.0-2.0 BLEU (free improvement, no training)

---

### 6.2 Checkpoint Averaging ‚≠ê PRIORITY 1

**Simpler than ensemble**: Average model weights

**Implementation**:
```python
# scripts/average_checkpoints.py (NEW FILE)
import torch

def average_checkpoints(checkpoint_paths, output_path):
    """
    Average weights from multiple checkpoints.

    Args:
        checkpoint_paths: List of checkpoint paths
        output_path: Path to save averaged checkpoint
    """
    print(f"Averaging {len(checkpoint_paths)} checkpoints...")

    # Load first checkpoint as base
    averaged_state = torch.load(checkpoint_paths[0], map_location='cpu')
    averaged_params = averaged_state['model_state_dict']

    # Add parameters from other checkpoints
    for path in checkpoint_paths[1:]:
        state = torch.load(path, map_location='cpu')
        params = state['model_state_dict']

        for key in averaged_params.keys():
            averaged_params[key] += params[key]

    # Divide by number of checkpoints
    for key in averaged_params.keys():
        averaged_params[key] /= len(checkpoint_paths)

    # Save
    averaged_state['model_state_dict'] = averaged_params
    torch.save(averaged_state, output_path)
    print(f"Saved averaged checkpoint to {output_path}")

# Usage:
# python scripts/average_checkpoints.py \
#   --checkpoints checkpoints/checkpoint_epoch{25,26,27,28,29}.pt \
#   --output checkpoints/averaged_best.pt
```

**Expected Impact**: +0.3-0.5 BLEU (even easier than ensemble)

---

### 6.3 Minimum Length Constraint üîß PRIORITY 3

**Problem**: Model sometimes generates too-short outputs

**Implementation**:
```python
# src/inference/beam_search.py
def beam_search(..., min_length=5):
    # In beam expansion
    if len(hypothesis.tokens) < min_length:
        # Block EOS token
        log_probs[eos_idx] = float('-inf')
```

**Config**:
```python
# config/transformer_config.py
min_decode_length = 5  # Minimum output length
```

---

### 6.4 Length Normalization Tuning üîß

**Current**: length_penalty = 0.6

**Experiment**: Try different values

```python
# config/transformer_config.py
length_penalty = 0.7  # Try 0.5, 0.6, 0.7, 0.8
```

**Effect**:
- Lower (0.5): Favors shorter outputs
- Higher (0.8): Favors longer outputs

---

### 6.5 Nucleus (Top-p) Sampling üÜï OPTIONAL

**Alternative to greedy/beam**:

```python
# src/inference/nucleus_sampling.py (NEW FILE)
def top_p_sampling(logits, top_p=0.9, temperature=1.0):
    """
    Nucleus sampling (top-p).

    Args:
        logits: [vocab_size]
        top_p: Cumulative probability threshold (0.9 = top 90% probability mass)
        temperature: Sampling temperature
    """
    # Apply temperature
    logits = logits / temperature
    probs = torch.softmax(logits, dim=-1)

    # Sort probabilities
    sorted_probs, sorted_indices = torch.sort(probs, descending=True)
    cumulative_probs = torch.cumsum(sorted_probs, dim=-1)

    # Remove tokens below threshold
    sorted_indices_to_remove = cumulative_probs > top_p
    sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()
    sorted_indices_to_remove[..., 0] = 0

    # Set removed token probs to 0
    sorted_probs[sorted_indices_to_remove] = 0
    sorted_probs = sorted_probs / sorted_probs.sum()  # Renormalize

    # Sample
    token_id = torch.multinomial(sorted_probs, 1)
    return sorted_indices[token_id].item()
```

---

## 7. Data Quality

### 7.1 Advanced Data Cleaning ‚≠ê PRIORITY 1

**Current issues**: Raw data may contain noise

**Recommended cleaning**:

```python
# scripts/clean_data.py (NEW FILE or enhance split_data.py)
import re
from bs4 import BeautifulSoup

class DataCleaner:
    """Advanced data cleaning for parallel corpus."""

    def clean_text(self, text):
        """Apply cleaning rules."""
        # Remove HTML tags
        text = BeautifulSoup(text, "html.parser").get_text()

        # Remove URLs
        text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)

        # Remove excessive punctuation
        text = re.sub(r'[!?]{3,}', '!', text)
        text = re.sub(r'\.{3,}', '...', text)

        # Normalize whitespace
        text = re.sub(r'\s+', ' ', text).strip()

        # Remove control characters
        text = ''.join(char for char in text if char.isprintable() or char.isspace())

        return text

    def is_valid_pair(self, src, tgt):
        """Check if pair is valid."""
        # Length checks
        if len(src) < 5 or len(tgt) < 5:
            return False
        if len(src) > 500 or len(tgt) > 500:
            return False

        # Length ratio check
        ratio = len(src) / len(tgt)
        if ratio > 3.0 or ratio < 0.33:
            return False

        # Check for too many numbers (might be garbage)
        num_count_src = len(re.findall(r'\d', src))
        num_count_tgt = len(re.findall(r'\d', tgt))
        if num_count_src > len(src) * 0.5 or num_count_tgt > len(tgt) * 0.5:
            return False

        # Check for repeated characters (spam)
        if re.search(r'(.)\1{5,}', src) or re.search(r'(.)\1{5,}', tgt):
            return False

        return True

    def deduplicate(self, pairs):
        """Remove duplicate pairs."""
        seen = set()
        unique_pairs = []

        for src, tgt in pairs:
            key = (src.strip(), tgt.strip())
            if key not in seen:
                seen.add(key)
                unique_pairs.append((src, tgt))

        return unique_pairs
```

**Expected Impact**: +0.5-1.0 BLEU from cleaner data

---

### 7.2 Increase Vocabulary Size ‚≠ê PRIORITY 2

**Current**: 16k vocab

**Recommended**:
```python
# config/base_config.py
vocab_size = 32000  # Increase to 32k (common for production systems)
```

**Why**:
- Better coverage of Korean morphemes
- Fewer <unk> tokens
- Better handling of rare words

**Tradeoff**: Slightly larger model

**Expected Impact**: +0.3-0.5 BLEU

---

### 7.3 Try BPE Instead of Unigram üîß EXPERIMENT

**Current**: Unigram tokenization

**Alternative**:
```python
# config/base_config.py
spm_model_type = "bpe"  # Byte-Pair Encoding instead of unigram
```

**Why**: BPE is more deterministic, sometimes works better for certain language pairs

---

### 7.4 Subword Regularization üÜï ADVANCED

**Approach**: Use multiple segmentations during training

```python
# src/data/dataset.py
# During tokenization, use SentencePiece sampling:
tokenized = sp.SampleEncode(text, nbest_size=-1, alpha=0.1)
# Instead of deterministic:
tokenized = sp.Encode(text)
```

**Expected Impact**: Better robustness, +0.2-0.3 BLEU

---

## 8. Evaluation & Monitoring

### 8.1 Add Better Metrics ‚≠ê PRIORITY 2

**Current**: BLEU only

**Add**:

```python
# src/utils/metrics.py

def compute_chrf(hypothesis, reference):
    """
    Character-level F-score (chrF++).
    Better correlation with human judgment than BLEU.
    """
    from sacrebleu import CHRF
    chrf = CHRF()
    score = chrf.sentence_score(hypothesis, [reference])
    return score.score

def compute_comet(source, hypothesis, reference, model_path='wmt20-comet-da'):
    """
    COMET: Neural metric based on cross-lingual representations.
    State-of-the-art correlation with human judgments.
    """
    from comet import download_model, load_from_checkpoint

    model = load_from_checkpoint(download_model(model_path))
    data = [{'src': source, 'mt': hypothesis, 'ref': reference}]
    score = model.predict(data, batch_size=1, gpus=1)
    return score[0]

def compute_bertscore(hypothesis, reference):
    """BERTScore: Contextual embedding similarity."""
    from bert_score import score
    P, R, F1 = score([hypothesis], [reference], lang='en')
    return F1.item()
```

**Install**:
```bash
pip install sacrebleu comet-ml bert-score
```

---

### 8.2 Error Analysis Tools üÜï PRIORITY 2

```python
# src/utils/error_analysis.py (NEW FILE)
import re

class ErrorAnalyzer:
    """Analyze common translation errors."""

    def __init__(self):
        self.number_pattern = re.compile(r'\d+')

    def analyze_batch(self, sources, hypotheses, references):
        """
        Analyze a batch of translations.

        Returns:
            stats: Dict with error statistics
        """
        stats = {
            'repetition_errors': 0,
            'number_errors': 0,
            'length_errors': 0,
            'total': len(sources)
        }

        for src, hyp, ref in zip(sources, hypotheses, references):
            # Check repetition
            if self.has_repetition(hyp):
                stats['repetition_errors'] += 1

            # Check number accuracy
            if not self.numbers_match(src, hyp):
                stats['number_errors'] += 1

            # Check length
            if abs(len(hyp.split()) - len(ref.split())) > 5:
                stats['length_errors'] += 1

        return stats

    def has_repetition(self, text, ngram_size=3):
        """Check for repeated n-grams."""
        words = text.split()
        ngrams = [tuple(words[i:i+ngram_size]) for i in range(len(words)-ngram_size+1)]
        return len(ngrams) != len(set(ngrams))

    def numbers_match(self, source, hypothesis):
        """Check if numbers in source appear in hypothesis."""
        src_numbers = set(self.number_pattern.findall(source))
        hyp_numbers = set(self.number_pattern.findall(hypothesis))

        if not src_numbers:
            return True  # No numbers to match

        # Check if all source numbers appear in hypothesis
        return src_numbers.issubset(hyp_numbers)
```

**Usage in training**:
```python
# In trainer.py validation loop
analyzer = ErrorAnalyzer()
stats = analyzer.analyze_batch(sources, predictions, references)
logger.log({
    'repetition_rate': stats['repetition_errors'] / stats['total'],
    'number_error_rate': stats['number_errors'] / stats['total']
})
```

---

### 8.3 Attention Visualization üÜï PRIORITY 3

```python
# src/utils/visualization.py (NEW FILE)
import matplotlib.pyplot as plt
import seaborn as sns

def visualize_attention(src_tokens, tgt_tokens, attention_weights,
                       save_path='attention.png'):
    """
    Plot attention heatmap.

    Args:
        src_tokens: List of source tokens
        tgt_tokens: List of target tokens
        attention_weights: [tgt_len, src_len] attention matrix
        save_path: Where to save plot
    """
    fig, ax = plt.subplots(figsize=(10, 10))

    sns.heatmap(
        attention_weights,
        xticklabels=src_tokens,
        yticklabels=tgt_tokens,
        cmap='Blues',
        ax=ax
    )

    plt.xlabel('Source')
    plt.ylabel('Target')
    plt.title('Attention Weights')
    plt.tight_layout()
    plt.savefig(save_path)
    plt.close()
```

---

### 8.4 Gradient Monitoring üîß PRIORITY 3

```python
# src/training/trainer.py
def monitor_gradients(self, model):
    """Log gradient statistics."""
    total_norm = 0
    param_norms = {}

    for name, param in model.named_parameters():
        if param.grad is not None:
            param_norm = param.grad.data.norm(2).item()
            total_norm += param_norm ** 2
            param_norms[name] = param_norm

    total_norm = total_norm ** 0.5

    # Log to tensorboard
    self.writer.add_scalar('train/grad_norm', total_norm, self.global_step)

    # Warn if gradients exploding
    if total_norm > 10.0:
        print(f"Warning: Large gradient norm: {total_norm:.2f}")
```

---

## 9. Implementation Roadmap

### Phase 1: Quick Wins (1-2 days) ‚≠ê

**Implement immediately for maximum impact**:

1. **Config adjustments** (30 minutes)
   - [ ] Set `share_src_tgt_embed = False`
   - [ ] Set `dropout = 0.1`
   - [ ] Set `warmup_steps = 4000`
   - [ ] Set `learning_rate = 2.0`

2. **Repetition fixes** (1 hour)
   - [ ] Increase repetition penalty to 1.5
   - [ ] Increase repetition window to 30
   - [ ] Add config parameters

3. **Checkpoint averaging** (1 hour)
   - [ ] Create averaging script
   - [ ] Average last 3-5 checkpoints
   - [ ] Test on validation set

4. **Mixed precision training** (2 hours)
   - [ ] Add AMP to trainer
   - [ ] Test training speed

5. **Better metrics** (2 hours)
   - [ ] Install sacrebleu, chrF++
   - [ ] Add to evaluation loop

**Expected improvement**: +2-3 BLEU, 2x training speed

---

### Phase 2: Repetition Solutions (2-3 days) üîÅ

6. **N-gram blocking** (4 hours)
   - [ ] Implement `ngram_blocking.py`
   - [ ] Integrate into greedy_search.py
   - [ ] Integrate into beam_search.py
   - [ ] Add config flags

7. **Diverse beam search** (4 hours)
   - [ ] Modify beam_search.py for beam groups
   - [ ] Add diversity penalty
   - [ ] Test and tune

8. **Error analysis tools** (3 hours)
   - [ ] Create `error_analysis.py`
   - [ ] Add to validation loop
   - [ ] Log statistics

**Expected improvement**: 70-90% reduction in repetitions

---

### Phase 3: Number Solutions (3-5 days) üî¢

**Option A: Copy Mechanism** (HIGH IMPACT, HIGH EFFORT)

9. **Implement copy mechanism** (1-2 days)
   - [ ] Create `copy_mechanism.py`
   - [ ] Modify transformer architecture
   - [ ] Update loss computation
   - [ ] Retrain model

**Option B: Number Placeholder System** (MEDIUM IMPACT, MEDIUM EFFORT)

10. **Implement placeholder system** (1 day)
    - [ ] Create `number_replacement.py`
    - [ ] Modify dataset preprocessing
    - [ ] Retrain model

**Option C: Post-processing** (LOW IMPACT, LOW EFFORT)

11. **Number alignment** (4 hours)
    - [ ] Create `number_alignment.py`
    - [ ] Integrate into translator
    - [ ] Test accuracy

**Recommendation**: Start with Option C (quick), then implement Option A (best results)

**Expected improvement**: 70-80% reduction in number errors with copy mechanism

---

### Phase 4: Data & Architecture (1 week) üìä

12. **Data cleaning** (2 days)
    - [ ] Implement advanced cleaning
    - [ ] Remove duplicates
    - [ ] Filter noisy pairs
    - [ ] Recreate processed dataset

13. **Increase vocabulary** (1 day)
    - [ ] Retrain tokenizers with 32k vocab
    - [ ] Update datasets

14. **Gradient accumulation** (2 hours)
    - [ ] Add to trainer
    - [ ] Test effective batch size 512

15. **Pre-LN architecture** (1 day, optional)
    - [ ] Modify encoder/decoder
    - [ ] Retrain from scratch

**Expected improvement**: +1-2 BLEU from data quality

---

### Phase 5: Advanced (2+ weeks) üöÄ

16. **Ensemble decoding** (1 day)
    - [ ] Implement `ensemble.py`
    - [ ] Test with multiple checkpoints

17. **Back-translation** (1 week)
    - [ ] Train reverse model (En‚ÜíKo)
    - [ ] Generate synthetic data
    - [ ] Retrain with augmented data

18. **Curriculum learning** (3 days)
    - [ ] Implement curriculum dataset
    - [ ] Retrain with curriculum

19. **Better architecture** (1 week)
    - [ ] Relative positional encoding
    - [ ] Layer dropout
    - [ ] Test variants

**Expected improvement**: +2-3 BLEU cumulative

---

## Implementation Priority Summary

### Do FIRST (Phase 1) - Highest ROI

‚úÖ **Already Done**:
- Double dropout fix
- Cross-attention mask fix

üî¥ **Do Immediately**:
1. Config adjustments (30 min) ‚Üí +1-2 BLEU
2. Checkpoint averaging (1 hour) ‚Üí +0.5 BLEU
3. Mixed precision (2 hours) ‚Üí 2x speed
4. Repetition penalty increase (30 min) ‚Üí 40% fewer repetitions

**Total time**: ~4 hours
**Total impact**: +2-3 BLEU, 2x training speed, 40% fewer repetitions

---

### Do SECOND (Phase 2) - Repetition

5. N-gram blocking (4 hours) ‚Üí 70% fewer repetitions
6. Error analysis (3 hours) ‚Üí visibility into problems

**Total time**: 1 day
**Total impact**: Major reduction in repetitions

---

### Do THIRD (Phase 3) - Numbers

**Choose one**:
- Copy mechanism (2 days) ‚Üí 80% fewer number errors (BEST)
- Number placeholders (1 day) ‚Üí 50% fewer errors (GOOD)
- Post-processing (4 hours) ‚Üí 30% fewer errors (QUICK FIX)

**Recommendation**: Start with post-processing, then implement copy mechanism

---

### Do LATER (Phases 4-5) - Refinement

7. Data cleaning ‚Üí +1-2 BLEU
8. Ensemble ‚Üí +1-2 BLEU
9. Advanced architectures ‚Üí +2-3 BLEU

---

## References

### Papers

1. **Original Transformer**: "Attention Is All You Need" (Vaswani et al., 2017)
2. **Copy Mechanism**: "Get To The Point" (See et al., 2017)
3. **Pre-LN**: "On Layer Normalization in the Transformer Architecture" (Xiong et al., 2020)
4. **Diverse Beam Search**: "Diverse Beam Search" (Vijayakumar et al., 2018)

### Tools

- SentencePiece: https://github.com/google/sentencepiece
- SacreBLEU: https://github.com/mjpost/sacrebleu
- COMET: https://github.com/Unbabel/COMET
- Hugging Face Transformers: https://huggingface.co/transformers

---

## Monitoring & Success Metrics

### Track These Metrics

**During Training**:
- Training loss
- Validation loss
- Gradient norm
- Learning rate
- Training speed (samples/sec)

**During Evaluation**:
- BLEU score (primary metric)
- chrF++ score
- Repetition rate (% of outputs with repetitions)
- Number error rate (% of outputs with number mismatches)
- Average output length
- <unk> token rate

**Quality Checks**:
- Manual inspection of 50-100 random translations
- Attention visualization for problematic examples
- Error categorization (repetition, omission, hallucination)

---

## Conclusion

This document provides a comprehensive roadmap for improving your Korean-English NMT system. The recommendations are prioritized by impact and implementation difficulty.

**Key Takeaways**:
1. Start with quick wins (config + checkpoint averaging) for immediate +2-3 BLEU
2. Address repetitions with n-gram blocking (70-90% reduction)
3. Fix numbers with copy mechanism (80% reduction in errors)
4. Improve data quality and use ensemble for additional gains

**Expected Total Improvement**: +5-8 BLEU points with all optimizations

Good luck with your implementation!
